{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeneticCNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AjjuSingh/Machine-Learning/blob/master/GeneticCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jhuG7vSM2_HN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Importing Modules**"
      ]
    },
    {
      "metadata": {
        "id": "Do9NAmZM7joC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install deap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IUXfS_OfI7Ss",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from deap import base, creator, tools, algorithms\n",
        "from scipy.stats import bernoulli\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o6mJDflX3OKF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**dag.py**"
      ]
    },
    {
      "metadata": {
        "id": "arKXhnzW0-ye",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "from copy import copy, deepcopy\n",
        "from collections import deque\n",
        "\n",
        "try:\n",
        "    from collections import OrderedDict\n",
        "except:\n",
        "    from ordereddict import OrderedDict\n",
        "\n",
        "\n",
        "class DAGValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "class DAG(object):\n",
        "    \"\"\" Directed acyclic graph implementation. \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\" Construct a new DAG with no nodes or edges. \"\"\"\n",
        "        self.reset_graph()\n",
        "\n",
        "    def add_node(self, node_name, graph=None):\n",
        "        \"\"\" Add a node if it does not exist yet, or error out. \"\"\"\n",
        "        if not graph:\n",
        "            graph = self.graph\n",
        "        if node_name in graph:\n",
        "            raise KeyError('node %s already exists' % node_name)\n",
        "        graph[node_name] = set()\n",
        "\n",
        "    def add_node_if_not_exists(self, node_name, graph=None):\n",
        "        try:\n",
        "            self.add_node(node_name, graph=graph)\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    def delete_node(self, node_name, graph=None):\n",
        "        \"\"\" Deletes this node and all edges referencing it. \"\"\"\n",
        "        if not graph:\n",
        "            graph = self.graph\n",
        "        if node_name not in graph:\n",
        "            raise KeyError('node %s does not exist' % node_name)\n",
        "        graph.pop(node_name)\n",
        "\n",
        "        for node, edges in graph.items():\n",
        "            if node_name in edges:\n",
        "                edges.remove(node_name)\n",
        "\n",
        "    def delete_node_if_exists(self, node_name, graph=None):\n",
        "        try:\n",
        "            self.delete_node(node_name, graph=graph)\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    def add_edge(self, ind_node, dep_node, graph=None):\n",
        "        \"\"\" Add an edge (dependency) between the specified nodes. \"\"\"\n",
        "        if not graph:\n",
        "            graph = self.graph\n",
        "        if ind_node not in graph or dep_node not in graph:\n",
        "            raise KeyError('one or more nodes do not exist in graph')\n",
        "        test_graph = deepcopy(graph)\n",
        "        test_graph[ind_node].add(dep_node)\n",
        "        is_valid, message = self.validate(test_graph)\n",
        "        if is_valid:\n",
        "            graph[ind_node].add(dep_node)\n",
        "        else:\n",
        "            raise DAGValidationError()\n",
        "\n",
        "    def delete_edge(self, ind_node, dep_node, graph=None):\n",
        "        \"\"\" Delete an edge from the graph. \"\"\"\n",
        "        if not graph:\n",
        "            graph = self.graph\n",
        "        if dep_node not in graph.get(ind_node, []):\n",
        "            raise KeyError('this edge does not exist in graph')\n",
        "        graph[ind_node].remove(dep_node)\n",
        "\n",
        "    def rename_edges(self, old_task_name, new_task_name, graph=None):\n",
        "        \"\"\" Change references to a task in existing edges. \"\"\"\n",
        "        if not graph:\n",
        "            graph = self.graph\n",
        "        for node, edges in graph.items():\n",
        "\n",
        "            if node == old_task_name:\n",
        "                graph[new_task_name] = copy(edges)\n",
        "                del graph[old_task_name]\n",
        "\n",
        "            else:\n",
        "                if old_task_name in edges:\n",
        "                    edges.remove(old_task_name)\n",
        "                    edges.add(new_task_name)\n",
        "\n",
        "    def predecessors(self, node, graph=None):\n",
        "        \"\"\" Returns a list of all predecessors of the given node \"\"\"\n",
        "        if graph is None:\n",
        "            graph = self.graph\n",
        "        return [key for key in graph if node in graph[key]]\n",
        "\n",
        "    def downstream(self, node, graph=None):\n",
        "        \"\"\" Returns a list of all nodes this node has edges towards. \"\"\"\n",
        "        if graph is None:\n",
        "            graph = self.graph\n",
        "        if node not in graph:\n",
        "            raise KeyError('node %s is not in graph' % node)\n",
        "        return list(graph[node])\n",
        "\n",
        "    def all_downstreams(self, node, graph=None):\n",
        "        \"\"\"Returns a list of all nodes ultimately downstream\n",
        "        of the given node in the dependency graph, in\n",
        "        topological order.\"\"\"\n",
        "        if graph is None:\n",
        "            graph = self.graph\n",
        "        nodes = [node]\n",
        "        nodes_seen = set()\n",
        "        i = 0\n",
        "        while i < len(nodes):\n",
        "            downstreams = self.downstream(nodes[i], graph)\n",
        "            for downstream_node in downstreams:\n",
        "                if downstream_node not in nodes_seen:\n",
        "                    nodes_seen.add(downstream_node)\n",
        "                    nodes.append(downstream_node)\n",
        "            i += 1\n",
        "        return filter(lambda node: node in nodes_seen, self.topological_sort(graph=graph))\n",
        "\n",
        "    def all_leaves(self, graph=None):\n",
        "        \"\"\" Return a list of all leaves (nodes with no downstreams) \"\"\"\n",
        "        if graph is None:\n",
        "            graph = self.graph\n",
        "        return [key for key in graph if not graph[key]]\n",
        "\n",
        "    def from_dict(self, graph_dict):\n",
        "        \"\"\" Reset the graph and build it from the passed dictionary.\n",
        "\n",
        "        The dictionary takes the form of {node_name: [directed edges]}\n",
        "        \"\"\"\n",
        "\n",
        "        self.reset_graph()\n",
        "        for new_node in graph_dict.iterkeys():\n",
        "            self.add_node(new_node)\n",
        "        for ind_node, dep_nodes in graph_dict.items():\n",
        "            if not isinstance(dep_nodes, list):\n",
        "                raise TypeError('dict values must be lists')\n",
        "            for dep_node in dep_nodes:\n",
        "                self.add_edge(ind_node, dep_node)\n",
        "\n",
        "    def reset_graph(self):\n",
        "        \"\"\" Restore the graph to an empty state. \"\"\"\n",
        "        self.graph = OrderedDict()\n",
        "\n",
        "    def ind_nodes(self, graph=None):\n",
        "        \"\"\" Returns a list of all nodes in the graph with no dependencies. \"\"\"\n",
        "        if graph is None:\n",
        "            graph = self.graph\n",
        "        dependent_nodes = set(tuple(node) for dependents in graph.items() for node in dependents)\n",
        "        return [node for node in graph.keys() if node not in dependent_nodes]\n",
        "\n",
        "    def validate(self, graph=None):\n",
        "        \"\"\" Returns (Boolean, message) of whether DAG is valid. \"\"\"\n",
        "        graph = graph if graph is not None else self.graph\n",
        "        if len(self.ind_nodes(graph)) == 0:\n",
        "            return (False, 'no independent nodes detected')\n",
        "        try:\n",
        "            self.topological_sort(graph)\n",
        "        except ValueError:\n",
        "            return (False, 'failed topological sort')\n",
        "        return (True, 'valid')\n",
        "\n",
        "    def topological_sort(self, graph=None):\n",
        "        \"\"\" Returns a topological ordering of the DAG.\n",
        "\n",
        "        Raises an error if this is not possible (graph is not valid).\n",
        "        \"\"\"\n",
        "        if graph is None:\n",
        "            graph = self.graph\n",
        "\n",
        "        in_degree = {}\n",
        "        for u in graph:\n",
        "            in_degree[u] = 0\n",
        "\n",
        "        for u in graph:\n",
        "            for v in graph[u]:\n",
        "                in_degree[v] += 1\n",
        "\n",
        "        queue = deque()\n",
        "        for u in in_degree:\n",
        "            if in_degree[u] == 0:\n",
        "                queue.appendleft(u)\n",
        "\n",
        "        l = []\n",
        "        while queue:\n",
        "            u = queue.pop()\n",
        "            l.append(u)\n",
        "            for v in graph[u]:\n",
        "                in_degree[v] -= 1\n",
        "                if in_degree[v] == 0:\n",
        "                    queue.appendleft(v)\n",
        "\n",
        "        if len(l) == len(graph):\n",
        "            return l\n",
        "        else:\n",
        "            raise ValueError('graph is not acyclic')\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.graph)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VSO_lXYrI49Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist = input_data.read_data_sets(\"mnist_data/\", one_hot=True)\n",
        "train_imgs   = mnist.train.images[:500]\n",
        "train_labels = mnist.train.labels[:500]\n",
        "test_imgs    = mnist.test.images[:50]\n",
        "test_labels  = mnist.test.labels[:50]\n",
        "\n",
        "train_imgs = np.reshape(train_imgs,[-1,28,28,1])\n",
        "test_imgs = np.reshape(test_imgs,[-1,28,28,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vD44S4PLJ9if",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "STAGES = np.array([\"s1\",\"s2\",\"s3\"]) # S\n",
        "NUM_NODES = np.array([3,4,5]) # K\n",
        "\n",
        "L =  0 # genome length\n",
        "BITS_INDICES, l_bpi = np.empty((0,2),dtype = np.int32), 0 # to keep track of bits for each stage S\n",
        "for nn in NUM_NODES:\n",
        "    t = nn * (nn - 1)\n",
        "    BITS_INDICES = np.vstack([BITS_INDICES,[l_bpi, l_bpi + int(0.5 * t)]])\n",
        "    l_bpi = int(0.5 * t)\n",
        "    L += t\n",
        "L = int(0.5 * L)\n",
        "\n",
        "TRAINING_EPOCHS = 20\n",
        "BATCH_SIZE = 20\n",
        "TOTAL_BATCHES = train_imgs.shape[0] // BATCH_SIZE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jdA3KEjELbKZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weight_variable(weight_name, weight_shape):\n",
        "    return tf.Variable(tf.truncated_normal(weight_shape, stddev = 0.1),name = ''.join([\"weight_\", weight_name]))\n",
        "\n",
        "def bias_variable(bias_name,bias_shape):\n",
        "    return tf.Variable(tf.constant(0.01, shape = bias_shape),name = ''.join([\"bias_\", bias_name]))\n",
        "\n",
        "def linear_layer(x,n_hidden_units,layer_name):\n",
        "    n_input = int(x.get_shape()[1])\n",
        "    weights = weight_variable(layer_name,[n_input, n_hidden_units])\n",
        "    biases = bias_variable(layer_name,[n_hidden_units])\n",
        "    return tf.add(tf.matmul(x,weights),biases)\n",
        "\n",
        "def apply_convolution(x,kernel_height,kernel_width,num_channels,depth,layer_name):\n",
        "    weights = weight_variable(layer_name,[kernel_height, kernel_width, num_channels, depth])\n",
        "    biases = bias_variable(layer_name,[depth])\n",
        "    return tf.nn.relu(tf.add(tf.nn.conv2d(x, weights,[1,2,2,1],padding = \"SAME\"),biases)) \n",
        "\n",
        "def apply_pool(x,kernel_height,kernel_width,stride_size):\n",
        "    return tf.nn.max_pool(x, ksize=[1, kernel_height, kernel_width, 1], \n",
        "            strides=[1, 1, stride_size, 1], padding = \"SAME\")\n",
        "\n",
        "def add_node(node_name, connector_node_name, h = 5, w = 5, nc = 1, d = 1):\n",
        "    with tf.name_scope(node_name) as scope:\n",
        "        conv = apply_convolution(tf.get_default_graph().get_tensor_by_name(connector_node_name), \n",
        "                   kernel_height = h, kernel_width = w, num_channels = nc , depth = d, \n",
        "                   layer_name = ''.join([\"conv_\",node_name]))\n",
        "\n",
        "def sum_tensors(tensor_a,tensor_b,activation_function_pattern):\n",
        "    if not tensor_a.startswith(\"Add\"):\n",
        "        tensor_a = ''.join([tensor_a,activation_function_pattern])\n",
        "        \n",
        "    return tf.add(tf.get_default_graph().get_tensor_by_name(tensor_a),\n",
        "                 tf.get_default_graph().get_tensor_by_name(''.join([tensor_b,activation_function_pattern])))\n",
        "\n",
        "def has_same_elements(x):\n",
        "    return len(set(x)) <= 1\n",
        "\n",
        "'''This method will come handy to first generate DAG independent of Tensorflow, \n",
        "    afterwards generated graph can be used to generate Tensorflow graph'''\n",
        "def generate_dag(optimal_indvidual,stage_name,num_nodes):\n",
        "    # create nodes for the graph\n",
        "    nodes = np.empty((0), dtype = np.str)\n",
        "    for n in range(1,(num_nodes + 1)):\n",
        "        nodes = np.append(nodes,''.join([stage_name,\"_\",str(n)]))\n",
        "    \n",
        "    # initialize directed asyclic graph (DAG) and add nodes to it\n",
        "    dag = DAG()\n",
        "    for n in nodes:\n",
        "        dag.add_node(n)\n",
        "\n",
        "    # split best indvidual found via GA to identify vertices connections and connect them in DAG \n",
        "    edges = np.split(optimal_indvidual,np.cumsum(range(num_nodes - 1)))[1:]\n",
        "    v2 = 2\n",
        "    for e in edges:\n",
        "        v1 = 1\n",
        "        for i in e:\n",
        "            if i:\n",
        "                dag.add_edge(''.join([stage_name,\"_\",str(v1)]),''.join([stage_name,\"_\",str(v2)])) \n",
        "            v1 += 1\n",
        "        v2 += 1\n",
        "\n",
        "    # delete nodes not connected to anyother node from DAG\n",
        "    for n in nodes:\n",
        "        if len(dag.predecessors(n)) == 0 and len(dag.downstream(n)) == 0:\n",
        "            dag.delete_node(n)\n",
        "            nodes = np.delete(nodes, np.where(nodes == n)[0][0])\n",
        "    \n",
        "    return dag, nodes\n",
        "\n",
        "def generate_tensorflow_graph(individual,stages,num_nodes,bits_indices):\n",
        "    activation_function_pattern = \"/Relu:0\"\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    X = tf.placeholder(tf.float32, shape = [None,28,28,1], name = \"X\")\n",
        "    Y = tf.placeholder(tf.float32,[None,10],name = \"Y\")\n",
        "        \n",
        "    d_node = X\n",
        "    for stage_name,num_node,bpi in zip(stages,num_nodes,bits_indices):\n",
        "        indv = individual[bpi[0]:bpi[1]]\n",
        "\n",
        "        add_node(''.join([stage_name,\"_input\"]),d_node.name)\n",
        "        pooling_layer_name = ''.join([stage_name,\"_input\",activation_function_pattern])\n",
        "\n",
        "        if not has_same_elements(indv):\n",
        "            # ------------------- Temporary DAG to hold all connections implied by GA solution ------------- #  \n",
        "\n",
        "            # get DAG and nodes in the graph\n",
        "            dag, nodes = generate_dag(indv,stage_name,num_node) \n",
        "            # get nodes without any predecessor, these will be connected to input node\n",
        "            without_predecessors = dag.ind_nodes() \n",
        "            # get nodes without any successor, these will be connected to output node\n",
        "            without_successors = dag.all_leaves()\n",
        "\n",
        "            # ----------------------------------------------------------------------------------------------- #\n",
        "\n",
        "            # --------------------------- Initialize tensforflow graph based on DAG ------------------------- #\n",
        "\n",
        "            for wop in without_predecessors:\n",
        "                add_node(wop,''.join([stage_name,\"_input\",activation_function_pattern]))\n",
        "\n",
        "            for n in nodes:\n",
        "                predecessors = dag.predecessors(n)\n",
        "                if len(predecessors) == 0:\n",
        "                    continue\n",
        "                elif len(predecessors) > 1:\n",
        "                    first_predecessor = predecessors[0]\n",
        "                    for prd in range(1,len(predecessors)):\n",
        "                        t = sum_tensors(first_predecessor,predecessors[prd],activation_function_pattern)\n",
        "                        first_predecessor = t.name\n",
        "                    add_node(n,first_predecessor)\n",
        "                elif predecessors:\n",
        "                    add_node(n,''.join([predecessors[0],activation_function_pattern]))\n",
        "\n",
        "            if len(without_successors) > 1:\n",
        "                first_successor = without_successors[0]\n",
        "                for suc in range(1,len(without_successors)):\n",
        "                    t = sum_tensors(first_successor,without_successors[suc],activation_function_pattern)\n",
        "                    first_successor = t.name\n",
        "                add_node(''.join([stage_name,\"_output\"]),first_successor) \n",
        "            else:\n",
        "                add_node(''.join([stage_name,\"_output\"]),''.join([without_successors[0],activation_function_pattern])) \n",
        "\n",
        "            pooling_layer_name = ''.join([stage_name,\"_output\",activation_function_pattern])\n",
        "            # ------------------------------------------------------------------------------------------ #\n",
        "\n",
        "        d_node =  apply_pool(tf.get_default_graph().get_tensor_by_name(pooling_layer_name), \n",
        "                                 kernel_height = 16, kernel_width = 16,stride_size = 2)\n",
        "\n",
        "    shape = d_node.get_shape().as_list()\n",
        "    flat = tf.reshape(d_node, [-1, shape[1] * shape[2] * shape[3]])\n",
        "    logits = linear_layer(flat,10,\"logits\")\n",
        "    \n",
        "    xentropy =  tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y)\n",
        "    loss_function = tf.reduce_mean(xentropy)\n",
        "    optimizer = tf.train.AdamOptimizer().minimize(loss_function) \n",
        "    # accuracy = tf.reduce_mean(tf.cast( tf.equal(tf.argmax(tf.nn.softmax(logits),1), tf.argmax(Y,1)), tf.float32))\n",
        "    accuracy = tf.reduce_mean(xentropy)\n",
        "    \n",
        "    return  X, Y, optimizer, loss_function, accuracy\n",
        "\n",
        "def evaluateModel(individual):\n",
        "    score = 0.0\n",
        "    X, Y, optimizer, loss_function, accuracy = generate_tensorflow_graph(individual,STAGES,NUM_NODES,BITS_INDICES)\n",
        "    with tf.Session() as session:\n",
        "        tf.global_variables_initializer().run()\n",
        "        for epoch in range(TRAINING_EPOCHS):\n",
        "            for b in range(TOTAL_BATCHES):\n",
        "                offset = (epoch * BATCH_SIZE) % (train_labels.shape[0] - BATCH_SIZE)\n",
        "                batch_x = train_imgs[offset:(offset + BATCH_SIZE), :, :, :]\n",
        "                batch_y = train_labels[offset:(offset + BATCH_SIZE), :]\n",
        "                _, c = session.run([optimizer, loss_function],feed_dict={X: batch_x, Y : batch_y})\n",
        "                \n",
        "        score = session.run(accuracy, feed_dict={X: test_imgs, Y: test_labels})\n",
        "        print('Accuracy: ',score)\n",
        "    return score,"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NZ5QQFaQ78CZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load dataset file"
      ]
    },
    {
      "metadata": {
        "id": "3Kh4UQ8I7_Hh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "dataframePath, n_pop, n_gen = 'sample_data/data2.csv', 20, 6\n",
        "# read dataframe from csv\n",
        "df = pd.read_csv(dataframePath, sep=',')\n",
        "\n",
        "# encode labels column to numbers\n",
        "le = LabelEncoder()\n",
        "le.fit(df.iloc[:, -1])\n",
        "y = le.transform(df.iloc[:, -1])\n",
        "X = df.iloc[:, :-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJppx0XbByUw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fwLD457l8s-q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Fitness Function"
      ]
    },
    {
      "metadata": {
        "id": "5YrZjTAAXXul",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "population_size = 4\n",
        "num_generations = 6\n",
        "\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights = (1.0,))\n",
        "creator.create(\"Individual\", list , fitness = creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"binary\", bernoulli.rvs, 0.5)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.binary, n = L)\n",
        "toolbox.register(\"population\", tools.initRepeat, list , toolbox.individual)\n",
        "\n",
        "toolbox.register(\"mate\", tools.cxOrdered)\n",
        "toolbox.register(\"mutate\", tools.mutShuffleIndexes, indpb = 0.8)\n",
        "toolbox.register(\"select\", tools.selRoulette)\n",
        "toolbox.register(\"evaluate\", evaluateModel)\n",
        "popl = toolbox.population(n = population_size)\n",
        "hof = tools.HallOfFame(population_size * num_generations)\n",
        "\n",
        "# result = algorithms.eaSimple(popl, toolbox, cxpb = 0.4, mutpb = 0.05, ngen = num_generations, verbose = True)\n",
        "result = algorithms.eaSimple(popl, toolbox, cxpb = 0.4, mutpb = 0.5, ngen = num_generations, verbose = True)\n",
        "best_individuals = tools.selBest(popl, k = 3)\n",
        "for bi in best_individuals:\n",
        "    hof = bi\n",
        "\"\"\"\n",
        "\n",
        "n_population = 5\n",
        "n_generation = 6\n",
        "\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "# create toolbox\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"binary\", bernoulli.rvs, 0.5)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.binary, n= L)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"evaluate\", evaluateModel)\n",
        "toolbox.register(\"mate\", tools.cxOrdered)\n",
        "toolbox.register(\"mutate\", tools.mutShuffleIndexes, indpb=0.8)\n",
        "toolbox.register(\"select\", tools.selRoulette)\n",
        "\n",
        "# initialize parameters\n",
        "pop = toolbox.population(n=n_population)\n",
        "hof = tools.HallOfFame(n_population * n_generation)\n",
        "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "stats.register(\"avg\", np.mean)\n",
        "stats.register(\"min\", np.min)\n",
        "stats.register(\"max\", np.max)\n",
        "\n",
        "# genetic algorithm\n",
        "pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2,\n",
        "                               ngen=n_generation, stats=stats, halloffame=hof,\n",
        "                               verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "92pk7DnL_Fl3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bestIndividual(hof, X, y):\n",
        "    \"\"\"\n",
        "    Get the best individual\n",
        "    \"\"\"\n",
        "    maxAccurcy = 0.0\n",
        "    for individual in hof:\n",
        "        if(individual.fitness.values[0] > maxAccurcy):\n",
        "            maxAccurcy = individual.fitness.values\n",
        "            _individual = individual\n",
        "\n",
        "    _individualHeader = [list(X)[i] for i in range(\n",
        "        len(_individual)) if _individual[i] == 1]\n",
        "    return _individual.fitness.values, _individual, _individualHeader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UYUGxmS8AAr9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "accuracy, individual, header = bestIndividual(hof, X, y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wI7xi0Y2AC_z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Best Accuracy: \\t' + str(accuracy))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6MzMku86AGAQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Individual: \\t\\t' + str(individual))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PAR3EOYeALkJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Feature Subset\\t: ' + str(header))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l49UtGUxAOv0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(dataframePath, sep=',')\n",
        "\n",
        "# with feature subset\n",
        "X = df[header]\n",
        "\n",
        "X"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}